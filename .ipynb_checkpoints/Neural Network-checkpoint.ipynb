{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "# scipty.special for the sigmoid function expit()\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    # initialize the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        '''\n",
    "        link weight matrices, wih and who(weight links input to hidden, weight links hidden \n",
    "        to output). weights inside the arrays are w_i_j, where link is from node i to nod j \n",
    "        in the next layer.\n",
    "        \n",
    "        One way is using random number generator from ranges -0.5 to 0.5\n",
    "        \n",
    "        self.wih = (numpy.random.rand(self.hnodes, self.inodes)-0.5)\n",
    "        self.who = (numpy.random.rand(self.onodes, self.hnodes)-0.5)\n",
    "        \n",
    "        Another way is to use the normal distribution cruve to apply the rule of thumb that the \n",
    "        range of intialweights should decrease as the number of links into a node increases. \n",
    "        This way a overly large initial weight won't bias the activation function in a bias \n",
    "        direction. This function will create a normal distribution curve with values with a \n",
    "        standard deviation of 1/sqrt(number of incoming links). Meaning the average distance of \n",
    "        each value to the center (0.0) should be 1/sqrt(number of incoming links). As the number\n",
    "        of links into a node increase the average distance of each value to the center decreases. \n",
    "        In other words the range will get smaller. \n",
    "\n",
    "        numpy.random.normal(center of distribution, standard deviation, size of numpy array)\n",
    "        \n",
    "        input->hidden->output\n",
    "        \n",
    "        where...\n",
    "        \n",
    "        center of distribution - 0.0\n",
    "        standard deviation - 1/sqrt(number of incoming links) or (number of incoming links)^-0.5\n",
    "        size of numpy array - \n",
    "        (number of hidden nodes, number of input nodes) \n",
    "        or \n",
    "        (number of output nodes, number of hidden nodes)\n",
    "        '''\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5),(self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5),(self.onodes, self.hnodes))\n",
    "        \n",
    "        '''\n",
    "        activation function is the sigmoid function. we used the lambda function as a shortcut \n",
    "        compared to using 'def' and returning x. The x will pass be to scipy.special.expit(x). \n",
    "        It will calculate and return the value in one line. We will call self.activation_function(x)\n",
    "        to use this.\n",
    "        '''\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        #calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        #calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        #calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        #calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target- actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        #hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "        \n",
    "        #update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_output * (1.0 - final_outputs)), numpy.transpose(hidden_outputs)) \n",
    "        \n",
    "        #update the wieghts for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs)) \n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        #calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        #calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        #calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        #calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "#learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32802717,  0.41067079, -0.58254081],\n",
       "       [ 0.47753499,  0.42328726, -0.57529493],\n",
       "       [ 0.09660271, -0.02894772, -0.12377656]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.normal(0,0.5,(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
